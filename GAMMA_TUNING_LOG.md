# Gamma参数调优日志

## 问题描述

v2.2/v2.3添加了等待乘客惩罚（gamma参数）来解决Q值学习失败问题，但导致发车次数过多。

## 调优历史

| 版本 | gamma | 发车次数 | 上行等待 | 下行等待 | 状态 |
|------|-------|---------|---------|---------|------|
| v2.0 | 无 | 72次 | 6.85分钟 | 19.56分钟 | ❌ Q值学习失败 |
| v2.2 | 0.15 | 102次 | 5.06分钟 | 7.08分钟 | ⚠️ 发车过多 |
| v2.3 | 0.08 | 93次 | 5.23分钟 | 7.94分钟 | ⚠️ 仍然过多 |
| v2.4 | 0.06 | ? | ? | ? | 🔄 训练中 |

## 目标

- 发车次数：70-76次（目标73）
- 上行等待：< 5.5分钟
- 下行等待：< 9.0分钟
- 滞留乘客：0人

## Gamma参数的作用

```python
# 等待乘客惩罚
waiting_penalty = min(waiting_passengers / 50.0, 1.0) * gamma

# 不发车时：额外惩罚
r -= waiting_penalty

# 发车时：小额奖励
r += waiting_penalty * 0.3
```

**效果**：
- gamma越大 → 惩罚越强 → 发车越频繁
- gamma越小 → 惩罚越弱 → 发车越少

## 线性插值计算

基于v2.2和v2.3的结果：

```
gamma=0.15 → 102次
gamma=0.08 → 93次

斜率 = (102-93)/(0.15-0.08) = 9/0.07 ≈ 128.6

目标73次，从93次需要减少20次：
Δgamma = 20/128.6 ≈ 0.016

gamma_v2.4 = 0.08 - 0.016 ≈ 0.064 ≈ 0.06
```

## 备选方案

如果v2.4 (gamma=0.06) 仍不理想：

### 方案A：继续微调gamma
- 如果发车>76次：降低到0.05
- 如果发车<70次：提高到0.07

### 方案B：调整归一化阈值
```python
# 当前：50人
waiting_penalty = min(waiting_passengers / 50.0, 1.0) * gamma

# 改为：70人（让惩罚更晚生效）
waiting_penalty = min(waiting_passengers / 70.0, 1.0) * gamma
```

### 方案C：非线性惩罚
```python
# 使用平方根，让惩罚增长更缓慢
waiting_penalty = (waiting_passengers / 50.0) ** 0.5 * gamma
```

### 方案D：分段惩罚
```python
# 只有等待乘客>阈值时才惩罚
if waiting_passengers > 30:
    waiting_penalty = min((waiting_passengers - 30) / 50.0, 1.0) * gamma
else:
    waiting_penalty = 0
```

## 风险评估

### 低风险 ✅
- 仅调整单个参数
- 可快速回滚
- 不影响核心逻辑

### 需要监控 ⚠️
- 等待时间是否显著增加
- Q值学习是否仍然有效
- 是否出现滞留乘客

## 成功标准

v2.4被认为成功，如果：
1. ✅ 发车次数：70-76次
2. ✅ 上行等待：< 5.5分钟
3. ✅ 下行等待：< 9.0分钟
4. ✅ 滞留乘客：0人
5. ✅ Q值学习正常（推理时能发车）

## 下一步

- 等待v2.4训练完成（约2分钟）
- 分析结果
- 如果成功，进行最终验证
- 如果失败，尝试备选方案
