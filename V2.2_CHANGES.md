# DRL-TSBC v2.2 改进说明

## 版本信息
- **版本**: v2.2
- **日期**: 2025-10-18
- **目标**: 解决Q值学习失败和下行等待时间过高问题

## 核心问题

### v2.0/v2.1的问题
1. **Q值学习失败** ⚠️ 严重
   - 训练时靠epsilon=0.1随机探索发车（70-80次）
   - Q网络学习到"不发车"的Q值最高（3.03 vs 1-2）
   - 推理时epsilon=0，完全依赖错误的Q值，导致不发车（0次）

2. **下行等待时间过高** ⚠️ 严重
   - 下行19.56分钟 vs 上行6.85分钟（2.85倍）
   - 论文目标：下行3.8分钟，上行3.7分钟（几乎相等）

3. **等待乘客严重累积**
   - 上行平均1705人，下行平均1338人
   - 说明发车时机严重不对

## v2.2 改进方案

### 改进1: 添加等待乘客强惩罚 🔥

**目标**: 解决Q值学习失败问题

**实现**:
```python
# 在严格论文公式基础上，添加等待乘客惩罚
gamma = 0.15  # 等待乘客惩罚系数
waiting_penalty = min(waiting_passengers / 50.0, 1.0) * gamma

# 不发车时：额外惩罚
r -= waiting_penalty

# 发车时：小额奖励（鼓励缓解等待压力）
r += waiting_penalty * 0.3
```

**原理**:
- 论文公式中的等待时间惩罚（ω×W）是累积的，反应慢
- 直接惩罚当前等待乘客数，让网络立即感知到不发车的代价
- 发车时给予小额奖励，强化"发车能缓解等待"的学习

### 改进2: 增加下行方向权重 🔥

**目标**: 解决下行等待时间过高问题

**实现**:
```python
down_weight = 1.5

# 下行的等待乘客惩罚加权
r_down -= waiting_down_penalty * down_weight  # 不发车
r_down += waiting_down_penalty * 0.3 * down_weight  # 发车
```

**原理**:
- 下行客流模式不同（晚高峰突增）
- 增加下行权重，让网络更重视下行的等待问题
- 保持发车次数平衡的同时，优化下行发车时机

### 改进3: 调整等待乘客归一化 🔥

**实现**:
```python
# 从100人调整到50人作为归一化上限
waiting_penalty = min(waiting_passengers / 50.0, 1.0) * gamma
```

**原理**:
- 50人是更合理的"需要发车"阈值
- 让惩罚更早生效，避免等待乘客过度累积

## 保持不变的论文参数

严格遵循论文规范，以下参数**完全不变**：

| 参数 | 值 | 说明 |
|------|-----|------|
| ω (omega) | 1/1000 | 等待时间权重 |
| β (beta) | 0.2 | 滞留乘客惩罚 |
| ζ (zeta) | 0.002 | 发车次数差异权重 |
| µ (mu) | 5000 | 等待时间归一化参数 |
| δ (delta) | 200 | 发车次数归一化参数 |
| Tmin | 5分钟 | 最小发车间隔 |
| Tmax | 20分钟 | 最大发车间隔 |
| epsilon | 0.1 | 探索率 |
| gamma | 0.4 | 折扣因子 |
| batch_size | 64 | 批次大小 |
| buffer_size | 3000 | 经验池大小 |

## 奖励函数对比

### 论文原始公式（v2.1）
```python
# 不发车
r_up = 1 - x1 - ω×x2 - β×d + ζ×(c_down - c_up)
r_down = 1 - y1 - ω×y2 - β×d + ζ×(c_up - c_down)

# 发车
r_up = x3 - β×d - ζ×(c_down - c_up)
r_down = y3 - β×d - ζ×(c_up - c_down)
```

### v2.2增强公式
```python
# 不发车
r_up = 1 - x1 - ω×x2 - β×d + ζ×(c_down - c_up) - waiting_penalty
r_down = 1 - y1 - ω×y2 - β×d + ζ×(c_up - c_down) - waiting_penalty × 1.5

# 发车
r_up = x3 - β×d - ζ×(c_down - c_up) + waiting_penalty × 0.3
r_down = y3 - β×d - ζ×(c_up - c_down) + waiting_penalty × 0.3 × 1.5
```

**关键差异**:
- 保持论文公式的所有项不变
- 仅添加等待乘客惩罚项（独立的额外项）
- 下行方向的惩罚/奖励加权1.5倍

## 预期效果

### 短期目标（v2.2）
- ✅ 解决Q值学习失败：推理时能正常发车（70-80次）
- ✅ 降低等待乘客累积：< 100人
- 🎯 上行等待时间: < 5.5分钟（目标5.0）
- 🎯 下行等待时间: < 10.0分钟（目标8.0）
- 🎯 下行/上行比率: < 2.0（目标1.5）

### 中期目标（v2.3）
- 上行等待时间: < 4.5分钟
- 下行等待时间: < 6.0分钟
- 接近论文水平

### 长期目标（v3.0）
- 上行等待时间: ≈ 3.7分钟
- 下行等待时间: ≈ 3.8分钟
- 完全复现论文结果

## 训练配置

```python
episodes = 50  # 保持不变
epsilon = 0.1  # 保持不变
learning_rate = 0.001  # 保持不变
```

## 风险评估

### 低风险 ✅
- 添加等待乘客惩罚是独立的额外项
- 不影响论文公式的核心逻辑
- 可以通过调整gamma系数来控制影响

### 需要监控 ⚠️
- 发车次数是否仍然平衡（目标73次）
- 是否出现过度发车（>90次）
- 滞留乘客数是否增加

## 回滚方案

如果v2.2效果不佳，可以：
1. 降低gamma从0.15到0.10或0.05
2. 降低down_weight从1.5到1.2
3. 完全回退到v2.1（纯论文公式）

## 下一步计划

如果v2.2成功：
1. 微调gamma和down_weight参数
2. 考虑添加epsilon衰减
3. 考虑扩展状态空间（添加等待乘客数特征）

如果v2.2失败：
1. 深入分析状态特征计算
2. 检查环境模拟的正确性
3. 对比论文实现的其他细节
