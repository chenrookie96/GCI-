# 第2章：基于深度强化学习的双向动态公交时刻表排班算法总结

第2章主要介绍了基于深度强化学习的双向动态公交时刻表排班算法（DRL-TSBC）。该算法旨在解决公交时刻表排班问题，通过深度强化学习动态决策公交发车时间，并保证上下行方向发车次数相等。以下是算法的核心组成部分、公式及流程的详细总结。

## 算法概述

DRL-TSBC 将双向公交时刻表排班问题建模为马尔可夫决策过程（MDP），使用深度Q网络（DQN） 作为学习框架。算法以分钟为粒度模拟公交运营环境，根据实时状态（如客流、车辆利用率）决定是否发车，并通过奖励函数引导智能体优化乘客等待时间和发车平衡。

## 马尔可夫决策过程建模

### 1. 状态空间

状态空间用于描述环境特征，包括时间、客流和车辆信息。状态向量分为上行方向状态 x_m 和下行方向状态 y_m，结构相同。第 m 分钟的状态定义如下（以归一化形式表示，范围 [0,1]）：

#### 时间状态（全局状态）

$$
a_m^1 = \frac{t_h}{24}, \quad a_m^2 = \frac{t_m}{60}
$$

其中，t_h 为当前小时数，t_m 为当前分钟数。

#### 上行方向状态 x_m（下行 y_m 类似）

##### 满载率

反映公交车利用率。

$$
x_m^1 = \frac{C_{\max}^{m, up}}{C_{\max}}
$$

其中，$C_{\max}^{m, up}$ 是第 m 分钟上行方向最大断面客流，$C_{\max}$ 是公交车最大载客量（常数）。

##### 归一化乘客等待时间

$$
x_m^2 = \frac{W_m^{up}}{\mu}
$$

其中，$W_m^{up}$ 是上行方向乘客总等待时间，$\mu = 5000$ 为归一化参数。总等待时间计算为：

$$
W_m^{up} = \sum_{k=1}^{K} \sum_{i=1}^{l_m^k} (t_b^{m,i,k} - t_a^{m,i,k})
$$

$t_a^{m,i,k}$ 和 $t_b^{m,i,k}$ 分别表示乘客 i 到达站点 k 的时间和登上公交的时间，$l_m^k$ 是站点 k 的上车乘客数，K 为站点总数。

##### 客运容量利用率

$$
x_m^3 = \frac{o_m}{e_m}
$$

其中，$e_m$ 是公交车提供的总容量，$o_m$ 是实际消耗容量：

$$
\[ e_{m}=\alpha\times C\times(K-1) \]
\[ o_{m}=\sum_{k=1}^{K-1}\left(c_{m}^{k}+l_{m}^{k}-h_{m}^{k}\right) \]
$$

$\alpha = 1.5$ 为站立系数，C 为座位数，$c_m^k$ 是公交车在站点 k 的载客量。

##### 发车次数差异

用于引导上下行发车平衡。

$$
x_m^4 = \frac{c_m^{up} - c_m^{down}}{\delta}
$$

其中，$c_m^{up}$ 和 $c_m^{down}$ 分别为上下行已发车次数，$\delta = 200$ 为归一化参数。

下行方向状态公式
公式(2.11)至(2.14)：
$$
\[ y_{m}^{1}=C_{\max}^{m,\text{down}}/C_{\max} \quad (2.11) \]
\[ y_{m}^{2}=W_{m}^{\text{down}}/\mu \quad (2.12) \]
\[ y_{m}^{3}=o_{m}^{\text{down}}/e_{m}^{\text{down}} \quad (2.13) \]
\[ y_{m}^{4}=c_{m}^{\text{down}}/\delta \quad (2.14) \]
$$
### 2. 动作空间

动作为二元决策，表示每分钟是否发车：

$$
a = (a^{up}, a^{down})
$$

其中，$a^{up}, a^{down} \in \{0,1\}$（0表示不发车，1表示发车）。例如，$a=(1,0)$ 表示仅上行发车。

### 3. 奖励函数

奖励函数设计为引导减少等待时间、避免滞留乘客并平衡发车次数。总奖励为上下行奖励之和：

$$
r_m = r^{up} + r^{down}
$$

#### 上行奖励 $r^{up}$,下行 $r^{down}$ 



\[ r_{m}^{up}=\begin{cases} 1-(o_{m}^{up}/e_{m}^{up})-(\omega\times W_{m}^{up})-(\beta\times d_{m}^{up})+\zeta(c_{m}^{up}-c_{m}^{down}), & a^{up}=0 \\ (o_{m}^{up}/e_{m}^{up})-(\beta\times d_{m}^{up})-\zeta(c_{m}^{up}-c_{m}^{down}), & a^{up}=1 \end{cases} \quad (2.16) \]
\[ r_{m}^{down}=\begin{cases} 1-(o_{m}^{down}/e_{m}^{down})-(\omega\times W_{m}^{down})-(\beta\times d_{m}^{down})-\zeta(c_{m}^{up}-c_{m}^{down}), & a^{down}=0 \\ (o_{m}^{down}/e_{m}^{down})-(\beta\times d_{m}^{down})+\zeta(c_{m}^{up}-c_{m}^{down}), & a^{down}=1 \end{cases} \quad (2.17) \]

其中：
- $d_m$ 为滞留乘客数（因满载无法上车），$\beta = 0.2$ 为惩罚权重
- $a_{up}/a_{down}$ = 0/1：表示上/下行，不发车/发车
- $\omega$ 为等待时间惩罚参数（可调，默认值基于实验）
- $\zeta = 0.002$ 为发车次数平衡权重

## 深度强化学习算法（DQN）

DRL-TSBC 使用 DQN 解决高维状态空间问题。关键公式包括：

### Q值更新目标

$$
Q_{\text{target}} = r + \gamma \max_{a'} Q(s', a'; \theta_i^-)
$$

其中，$\gamma$ 为折扣因子，$\theta_i^-$ 为目标网络参数。

### 损失函数（均方误差）

$$
\mathcal{L}_i(\theta_i) = \mathbb{E} \left[ \left( Q_{\text{target}} - Q(s, a; \theta_i) \right)^2 \right]
$$

## 算法流程

\documentclass{article}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{xeCJK} % 用于中文支持
\setCJKmainfont{SimSun} % 设置中文字体，例如SimSun（宋体）

\begin{document}

% 算法2.1: DRL-TSBC的学习过程
\begin{algorithm}
\caption{DRL-TSBC的学习过程}
\label{alg:learning}
\begin{algorithmic}[1]
\REQUIRE 超参数: 经验池大小 $M$, 批次大小 $B$, 折扣因子 $\gamma$, 随机动作概率 $\epsilon$, 最大模拟次数 $E$, 预训练步数 $t_p$, 衰减率 $\beta$, 目标网络的更新率 $\alpha$
\ENSURE 训练好的DQN参数 $\theta$
\STATE 初始化经验池 $D$，主网络参数 $\theta$，目标网络参数 $\theta^- = \theta$
\STATE 使用 $\theta$ 初始化主网络 $Q(s, a; \theta)$, 并且使用 $\theta^-$ 初始化目标网络 $Q(s, a; \theta^-)$
\FOR{episode = 1 to $E$}
\STATE 初始化状态 $s$（包括时间、客流数据等）
\FOR{$m = 1$ to $T$} \COMMENT{$T$ 为一天的总分钟数}
\IF{随机数 $< \epsilon$}
\STATE 随机选择动作 $a = (a^{up}, a^{down}) \in \{(0,0), (0,1), (1,0), (1,1)\}$
\ELSE
\STATE $a = \arg\max_a Q(s, a; \theta)$ \COMMENT{使用主网络选择Q值最大的动作}
\ENDIF
\STATE \COMMENT{应用动作前检查发车间隔约束：如果 $I_m < T_{\min}$ 强制不发车，如果 $I_m > T_{\max}$ 强制发车}
\STATE 执行动作 $a$，观察奖励 $r$ 和下一个状态 $s'$
\STATE 存储经验 $(s, a, r, s')$ 到经验池 $D$
\IF{$|D| > M$} \COMMENT{如果经验池已满，删除最旧的经验}
\STATE 从 $D$ 中随机采样一个批次 $B$ 个经验 $(s_i, a_i, r_i, s'_i)$
\STATE 计算目标Q值：对于每个样本，$y_i = r_i + \gamma \cdot \max_{a'} Q(s'_i, a'; \theta^-)$
\STATE 计算损失函数 $\mathcal{L}(\theta) = \frac{1}{B} \sum_{i=1}^{B} (y_i - Q(s_i, a_i; \theta))^2$
\STATE 使用Adam优化器通过反向传播更新 $\theta$
\STATE 每隔固定步数更新目标网络：$\theta^- = \alpha \theta + (1 - \alpha) \theta^-$ \COMMENT{软更新}
\ENDIF
\STATE 更新状态 $s = s'$
\STATE 衰减 $\epsilon = \epsilon - \beta$ \COMMENT{减少探索率}
\ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

% 算法2.2: DRL-TSBC的推理过程
\begin{algorithm}
\caption{DRL-TSBC的推理过程}
\label{alg:inference}
\begin{algorithmic}[1]
\REQUIRE 训练好的DQN参数 $\theta$, 最大发车间隔 $T_{\max}$, 最小发车间隔 $T_{\min}$
\ENSURE 最终公交时刻表（发车时间点列表）
\STATE 初始化状态 $s$，发车时刻表为空
\FOR{$m = 1$ to $T$} \COMMENT{模拟一天中的每一分钟}
\STATE 使用主网络 $Q(s, a; \theta)$ 选择动作 $a = \arg\max_a Q(s, a; \theta)$
\STATE 检查发车间隔约束：如果当前时间与上次发车间隔 $I_m < T_{\min}$，则强制 $a = 0$（不发车）；如果 $I_m > T_{\max}$，则强制 $a = 1$（发车）
\STATE 如果 $a$ 指示发车，则将当前时间 $m$ 添加到发车时刻表
\STATE 更新状态到 $s'$（模拟乘客上下车和车辆移动）
\STATE 设置 $s = s'$
\ENDFOR
\STATE \COMMENT{后处理：调整时刻表以确保上下行发车次数相等}
\STATE 计算上行和下行方向的总发车次数 $c^{up}$ 和 $c^{down}$
\IF{$c^{up} \neq c^{down}$}
\STATE 确定发车次数较多的方向（例如上行）
\STATE 从该方向的时刻表中删除倒数第二个发车时间点
\STATE 以 $T_{\max}$ 为发车间隔，向前调整公交的发车时刻，直到不需要再进行调整，即满足发车间隔的约束
\ENDIF
\STATE 返回调整后的发车时刻表
\end{algorithmic}
\end{algorithm}

\end{document}

## 关键参数

- 发车间隔约束：$T_{\min}$（最小间隔）、$T_{\max}$（最大间隔）
- DQN 超参数：学习率 0.001，批次大小 64，经验池大小 $10^5$，折扣因子 $\gamma = 0.99$

## 总结

本算法通过综合状态设计和奖励函数，实现了动态调整时刻表并保证发车平衡。实验表明，DRL-TSBC 在离线与在线排班中均能降低乘客平均等待时间约12.1%，并有效应对客流突变。
